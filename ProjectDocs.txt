The program initiates by activating its listening capability for specific keywords. Upon detecting the keyword "computer," it starts an audio recording session. The recording continues until the keyword "reply" is heard, which signals the end of the recording. Additionally, if the keyword "snapshot" is detected during the recording, the program captures an image immediately.

After the recording ends, the audio file is sent to Assembly AI for transcription. The program then needs to decide how to handle this transcription based on the context of ongoing conversations. It checks if there is an existing thread that was recently interacted with (within the last 90 seconds). This check is facilitated by a variable that tracks the state change when a message is received back from the assistant.

Depending on whether a recent thread exists, the program either creates a new thread for the transcription or adds the transcription to the last active thread. After establishing where the transcription should go, it initiates a stream on the chosen thread to continue the interaction.

Finally, once a response is formulated based on the transcription and any further interaction within the thread, this response is sent to Eleven Labs, where it is converted into speech and played back to the user. This ensures a seamless auditory interaction, completing the cycle of voice command to action to verbal feedback, making the user experience interactive and engaging.

here are some code examples: 

from typing_extensions import override
from openai import AssistantEventHandler
 
# First, we create a EventHandler class to define
# how we want to handle the events in the response stream.
 
class EventHandler(AssistantEventHandler):    
  @override
  def on_text_created(self, text) -> None:
    print(f"\nassistant > ", end="", flush=True)
      
  @override
  def on_text_delta(self, delta, snapshot):
    print(delta.value, end="", flush=True)
      
  def on_tool_call_created(self, tool_call):
    print(f"\nassistant > {tool_call.type}\n", flush=True)
  
  def on_tool_call_delta(self, delta, snapshot):
    if delta.type == 'code_interpreter':
      if delta.code_interpreter.input:
        print(delta.code_interpreter.input, end="", flush=True)
      if delta.code_interpreter.outputs:
        print(f"\n\noutput >", flush=True)
        for output in delta.code_interpreter.outputs:
          if output.type == "logs":
            print(f"\n{output.logs}", flush=True)
 
# Then, we use the `create_and_stream` SDK helper 
# with the `EventHandler` class to create the Run 
# and stream the response.
 
with client.beta.threads.runs.create_and_stream(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="Please address the user as Jane Doe. The user has a premium account.",
  event_handler=EventHandler(),
) as stream:
  stream.until_done()

And here is how elevenlabs handles streaming:

import requests

url = "https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream"

payload = {
    "model_id": "<string>",
    "pronunciation_dictionary_locators": [
        {
            "pronunciation_dictionary_id": "<string>",
            "version_id": "<string>"
        }
    ],
    "text": "<string>",
    "voice_settings": {
        "similarity_boost": 123,
        "stability": 123,
        "style": 123,
        "use_speaker_boost": True
    }
}
headers = {"Content-Type": "application/json"}

response = requests.request("POST", url, json=payload, headers=headers)

print(response.text)
